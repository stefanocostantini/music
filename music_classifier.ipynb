{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import kapre\n",
    "import keras\n",
    "import os\n",
    "import soundfile as sf\n",
    "import time\n",
    "import progressbar\n",
    "import gc\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metadata to extract labels\n",
    "metadata = pd.read_csv('/data/music/musicnet/metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>composer</th>\n",
       "      <th>composition</th>\n",
       "      <th>movement</th>\n",
       "      <th>ensemble</th>\n",
       "      <th>source</th>\n",
       "      <th>transcriber</th>\n",
       "      <th>catalog_name</th>\n",
       "      <th>seconds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1727</td>\n",
       "      <td>Schubert</td>\n",
       "      <td>Piano Quintet in A major</td>\n",
       "      <td>2. Andante</td>\n",
       "      <td>Piano Quintet</td>\n",
       "      <td>European Archive</td>\n",
       "      <td>http://tirolmusic.blogspot.com/</td>\n",
       "      <td>OP114</td>\n",
       "      <td>447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1728</td>\n",
       "      <td>Schubert</td>\n",
       "      <td>Piano Quintet in A major</td>\n",
       "      <td>3. Scherzo: Presto</td>\n",
       "      <td>Piano Quintet</td>\n",
       "      <td>European Archive</td>\n",
       "      <td>http://tirolmusic.blogspot.com/</td>\n",
       "      <td>OP114</td>\n",
       "      <td>251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1729</td>\n",
       "      <td>Schubert</td>\n",
       "      <td>Piano Quintet in A major</td>\n",
       "      <td>4. Andantino - Allegretto</td>\n",
       "      <td>Piano Quintet</td>\n",
       "      <td>European Archive</td>\n",
       "      <td>http://tirolmusic.blogspot.com/</td>\n",
       "      <td>OP114</td>\n",
       "      <td>444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1730</td>\n",
       "      <td>Schubert</td>\n",
       "      <td>Piano Quintet in A major</td>\n",
       "      <td>5. Allegro giusto</td>\n",
       "      <td>Piano Quintet</td>\n",
       "      <td>European Archive</td>\n",
       "      <td>http://tirolmusic.blogspot.com/</td>\n",
       "      <td>OP114</td>\n",
       "      <td>368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1733</td>\n",
       "      <td>Schubert</td>\n",
       "      <td>Piano Sonata in A major</td>\n",
       "      <td>2. Andantino</td>\n",
       "      <td>Solo Piano</td>\n",
       "      <td>Museopen</td>\n",
       "      <td>Segundo G. Yogore</td>\n",
       "      <td>D959</td>\n",
       "      <td>546</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  composer               composition                   movement  \\\n",
       "0  1727  Schubert  Piano Quintet in A major                 2. Andante   \n",
       "1  1728  Schubert  Piano Quintet in A major         3. Scherzo: Presto   \n",
       "2  1729  Schubert  Piano Quintet in A major  4. Andantino - Allegretto   \n",
       "3  1730  Schubert  Piano Quintet in A major          5. Allegro giusto   \n",
       "4  1733  Schubert   Piano Sonata in A major               2. Andantino   \n",
       "\n",
       "        ensemble            source                      transcriber  \\\n",
       "0  Piano Quintet  European Archive  http://tirolmusic.blogspot.com/   \n",
       "1  Piano Quintet  European Archive  http://tirolmusic.blogspot.com/   \n",
       "2  Piano Quintet  European Archive  http://tirolmusic.blogspot.com/   \n",
       "3  Piano Quintet  European Archive  http://tirolmusic.blogspot.com/   \n",
       "4     Solo Piano          Museopen                Segundo G. Yogore   \n",
       "\n",
       "  catalog_name  seconds  \n",
       "0        OP114      447  \n",
       "1        OP114      251  \n",
       "2        OP114      444  \n",
       "3        OP114      368  \n",
       "4         D959      546  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Schubert', 'Bach', 'Beethoven']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construct a dictionary id --> composer, which will be used later to set the labels for all sound snippets\n",
    "composers = pd.Series(metadata.composer.values,index=metadata.id).to_dict()\n",
    "examples = [1755, 2211, 2368]\n",
    "[composers.get(example) for example in examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dataset\n",
    "# First we define a function to load the audio snippets and to create the labels\n",
    "def load_audio(path, downsampling_rate, duration_sec, composers):\n",
    "        '''Requires the soundfile package, imported as sf '''\n",
    "        files = os.listdir(path)\n",
    "        message = \"Processing {0} audio files...\".format(len(files))\n",
    "        print(message)\n",
    "        message = \"Downsampling dataset to {0}% (equivalent to {1} audio files).\".format(downsampling_rate*100, int(downsampling_rate * len(files)) )\n",
    "        print(message)\n",
    "        \n",
    "        ds = np.random.choice(files, int(len(files) * downsampling_rate), replace=False)\n",
    "        \n",
    "        # Initialise empty arrays\n",
    "        data = np.zeros((len(ds), 1, 88200))\n",
    "        labels = np.zeros(len(ds), dtype = \"<U10\")\n",
    "        \n",
    "        with progressbar.ProgressBar(max_value=len(ds)) as bar:\n",
    "            \n",
    "            for i, file in enumerate(ds):\n",
    "                # load and process file, then add to array\n",
    "                audio_clip, sr = sf.read(path + file)\n",
    "                audio_clip = audio_clip[:int(sr*duration_sec)]\n",
    "                audio_clip = audio_clip[np.newaxis, :]\n",
    "                data[i, :audio_clip.shape[0],:audio_clip.shape[1]] = audio_clip\n",
    "                audio_clip = None\n",
    "            \n",
    "                # look up label and add to array\n",
    "                file_id = file.split(\"-\")[0]\n",
    "                label = composers[int(file_id)]\n",
    "                labels[i] = label \n",
    "                label = None\n",
    "                bar.update(i)\n",
    "            \n",
    "        return labels, data\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These sections can be skipped as there's no need to re-create these variables:\n",
    "# they can be loaded from disk.\n",
    "#\n",
    "# Load audio files, process them and create dataset and labels\n",
    "labels, data = load_audio(\"/data/music/musicnet/data_chunks/\", 0.1, 2, composers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn labels to categorical (one-hot encoding)\n",
    "labels_pd = pd.DataFrame(labels)\n",
    "onehot = pd.get_dummies(labels_pd)\n",
    "targets = onehot.as_matrix()\n",
    "targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store datasets as hdf5 files\n",
    "all_data = h5py.File('/data/music/musicnet/hdf5/data.hdf5', 'w')\n",
    "all_data.create_dataset('data', data=data)\n",
    "all_data.create_dataset('targets', data=targets)\n",
    "all_data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove variables and release all memory\n",
    "data = None\n",
    "targets = None\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training, validation and test datasets\n",
    "all_data = h5py.File('/data/music/musicnet/hdf5/data.hdf5', 'r')\n",
    "list(all_data.keys())\n",
    "data = all_data['data']\n",
    "targets = all_data['targets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOW DELETE data.hdf5 from disk if space is a constraint ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get indices for random sampling of datasets\n",
    "indices = np.random.permutation(data.shape[0])\n",
    "training_test_split = 0.8\n",
    "size_training_bf_validation = int(len(indices) * training_test_split)\n",
    "training_validation_split = 0.8\n",
    "size_training = int(size_training_bf_validation * training_validation_split)\n",
    "size_validation = size_training_bf_validation - size_training\n",
    "size_test = len(indices) - size_training - size_validation\n",
    "\n",
    "training_idx = indices[:size_training]\n",
    "validation_idx = indices[size_training:(size_training+size_validation)]\n",
    "test_idx = indices[(size_training+size_validation):(size_training+size_validation+size_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate each dataset, save as hdf5 and release memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "training_data = data[np.sort(training_idx),:]\n",
    "training_targets = targets[np.sort(training_idx),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = h5py.File('/data/music/musicnet/hdf5/training.hdf5', 'w')\n",
    "training.create_dataset('data', data=training_data)\n",
    "training.create_dataset('targets', data=training_targets)\n",
    "training.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = None\n",
    "training_targets = None\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validatation\n",
    "validation_data = data[np.sort(validation_idx),:]\n",
    "validation_targets = targets[np.sort(validation_idx),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = h5py.File('/data/music/musicnet/hdf5/validation.hdf5', 'w')\n",
    "validation.create_dataset('data', data=validation_data)\n",
    "validation.create_dataset('targets', data=validation_targets)\n",
    "validation.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = None\n",
    "validation_targets = None\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "test_data = data[np.sort(test_idx),:]\n",
    "test_targets = targets[np.sort(test_idx),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = h5py.File('/data/music/musicnet/hdf5/test.hdf5', 'w')\n",
    "test.create_dataset('data', data=test_data)\n",
    "test.create_dataset('targets', data=test_targets)\n",
    "test.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = None\n",
    "test_targets = None\n",
    "data = None\n",
    "targets = None\n",
    "training = None\n",
    "validation = None\n",
    "test = None\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONVOLUTIONAL NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a model whose first layer is a mel-spectrogram (from Kapre)\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from kapre.time_frequency import Melspectrogram\n",
    "from kapre.utils import Normalization2D\n",
    "from kapre.augmentation import AdditiveNoise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code inspired by https://github.com/keunwoochoi/kapre\n",
    "input_shape = (1, 88200)\n",
    "sr = 44100\n",
    "\n",
    "model = Sequential()\n",
    "# A mel-spectrogram layer\n",
    "model.add(Melspectrogram(n_dft=512, n_hop=256, input_shape=input_shape,\n",
    "                         padding='same', sr=sr, n_mels=64,\n",
    "                         fmin=0.0, fmax=sr/2, power_melgram=1.0,\n",
    "                         return_decibel_melgram=False, trainable_fb=False,\n",
    "                         trainable_kernel=False,\n",
    "                         name='trainable_stft'))\n",
    "# Add some white noise\n",
    "model.add(AdditiveNoise(power=0.2))\n",
    "# Normalise it per-frequency\n",
    "model.add(Normalization2D(str_axis='batch')) # or 'channel', 'time', 'batch', 'data_sample'\n",
    "# Add convolution layers\n",
    "model.add(layers.Conv2D(16, (3, 3), activation = 'relu'))\n",
    "model.add(layers.Conv2D(16, (3, 3), activation = 'relu'))\n",
    "model.add(layers.MaxPooling2D((3,3)))\n",
    "model.add(layers.Dropout(rate=0.1))\n",
    "\n",
    "model.add(layers.Conv2D(32, (3, 3), activation = 'relu'))\n",
    "model.add(layers.Conv2D(32, (3, 3), activation = 'relu'))\n",
    "model.add(layers.MaxPooling2D((3,3)))\n",
    "model.add(layers.Dropout(rate=0.1))\n",
    "\n",
    "model.add(layers.Conv2D(64, (3, 3), activation = 'relu'))\n",
    "model.add(layers.MaxPooling2D((3,3)))\n",
    "model.add(layers.Dropout(rate=0.1))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training and validation data from hdf5\n",
    "training = h5py.File('/data/music/musicnet/hdf5/training.hdf5', 'r')\n",
    "validation = h5py.File('/data/music/musicnet/hdf5/validation.hdf5', 'r')\n",
    "\n",
    "list(training.keys())\n",
    "training_data = training['data']\n",
    "training_targets = training['targets']\n",
    "\n",
    "list(validation.keys())\n",
    "validation_data = validation['data']\n",
    "validation_targets = validation['targets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "from keras import optimizers\n",
    "model.compile(optimizer = 'adam',\n",
    "              loss = 'categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3941 samples, validate on 986 samples\n",
      "Epoch 1/20\n",
      "3941/3941 [==============================] - 10s 2ms/step - loss: 1.0461 - acc: 0.6265 - val_loss: 1.8940 - val_acc: 0.5527\n",
      "Epoch 2/20\n",
      "3941/3941 [==============================] - 9s 2ms/step - loss: 1.0317 - acc: 0.6316 - val_loss: 1.9171 - val_acc: 0.5223\n",
      "Epoch 3/20\n",
      "3941/3941 [==============================] - 9s 2ms/step - loss: 1.0261 - acc: 0.6440 - val_loss: 1.8531 - val_acc: 0.5193\n",
      "Epoch 4/20\n",
      "3941/3941 [==============================] - 9s 2ms/step - loss: 1.0180 - acc: 0.6338 - val_loss: 1.8721 - val_acc: 0.5223\n",
      "Epoch 5/20\n",
      "3941/3941 [==============================] - 9s 2ms/step - loss: 1.0101 - acc: 0.6427 - val_loss: 1.8862 - val_acc: 0.5385\n"
     ]
    }
   ],
   "source": [
    "file_path=\"/home/ubuntu/code/trained_model/music/music-weights-improvement-{epoch:02d}-{val_loss:.4f}-{val_acc:.2f}.hdf5\"\n",
    "\n",
    "callbacks_list = [\n",
    "        keras.callbacks.EarlyStopping(monitor='acc', patience=2,),\n",
    "        keras.callbacks.ModelCheckpoint(filepath=file_path, monitor='val_loss', save_best_only=True,),\n",
    "        keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=7,)]\n",
    "\n",
    "history = model.fit(training_data, \n",
    "                    training_targets,\n",
    "                    epochs = 20,\n",
    "                    batch_size = 150,\n",
    "                    callbacks=callbacks_list,\n",
    "                    validation_data=(validation_data, validation_targets),\n",
    "                    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "# Total Loss\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Joint - Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.subplots(1)\n",
    "plt.plot(epochs, acc, 'bo', label='Training accuracy')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
    "plt.title('Gender - Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do test predictions and compare performance with baseline\n",
    "test = h5py.File('/data/music/musicnet/hdf5/test.hdf5', 'r')\n",
    "\n",
    "list(test.keys())\n",
    "test_data = test['data']\n",
    "test_targets = test['targets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = model.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predicted_labels = np.argmax(test_predictions, axis=1)\n",
    "test_target_labels = np.argmax(test_targets, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report\n",
    "print('Accuracy ', accuracy_score(test_target_labels, test_predicted_labels))\n",
    "print(classification_report(test_target_labels, test_predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with baseline classifier (in this case we pick the most popular class)\n",
    "most_popular = np.argmax(np.bincount(test_target_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_prediction = np.repeat(most_popular, test_target_labels.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy ', accuracy_score(test_target_labels, baseline_prediction))\n",
    "print(classification_report(test_target_labels, baseline_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_p36]",
   "language": "python",
   "name": "conda-env-tensorflow_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
